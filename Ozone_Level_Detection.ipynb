{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHgd1NM12hRI"
   },
   "source": [
    "# Ozone Layer Detection\n",
    "\n",
    "In this notebook, we use three methods to analyze the [Ozone Level Detection dataset, obtained from UCI](https://archive.ics.uci.edu/ml/datasets/Ozone+Level+Detection).\n",
    "\n",
    "\n",
    "\n",
    "*   Support Vector Machine\n",
    "*   K-Nearest Neighbour\n",
    "*   Multilayer Perceptron Feed-Forward Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fHozjBo0fLX"
   },
   "outputs": [],
   "source": [
    "# get the files\n",
    "\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.data\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/eighthr.names\n",
    "\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.data\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/ozone/onehr.names\n",
    "  \n",
    "!wget https://github.com/aaakashkumar/Ozone-Level-Detection/raw/master/eighthr.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWZUxtpT5yvD"
   },
   "source": [
    "## Imports\n",
    "\n",
    "It's a good idea to make all the necessary imports in the beginning of the notebook. However, we make imports catering to the specific methods in their respective sections, for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8NPk86t6JQU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNbw8QG-324d"
   },
   "source": [
    "## Data Cleanup\n",
    "\n",
    "The dataset contains 2536 rows of values, and 73 attributes. Out of these, 687 rows contain missing values represented as \"`?`''.\n",
    "\n",
    "The `eighthr.data` was converted to CSV using [OpenRefine](http://openrefine.org/), and respective column names were added from `eighthr.names` file.\n",
    "\n",
    "Here we read the csv file and replace the missing values (`?`) by row means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97jv9cMW8pgz"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('eighthr.csv')\n",
    "df_original = pd.read_csv('eighthr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp7E1yyc9Hjw"
   },
   "source": [
    "### Examining the dataset\n",
    "It's a good idea to get to know the data a little bit before working with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "id": "-KynIuvK9TWX",
    "outputId": "cc8dd6fb-3183-417e-943e-11620a3ab56b"
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HcPQ07v-Em4"
   },
   "source": [
    "We'll print out a quick summary of a few useful statistics on each column.\n",
    "\n",
    "This will include things like mean, standard deviation, max, min, and various quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "5dglWQ8y90S0",
    "outputId": "db1a7adb-1658-49e1-eca1-26b2c55b5fc7"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV6DpmJ5-hWY"
   },
   "source": [
    "We remove the `Date` column, as it is not useful for prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzY4Vec7-okE"
   },
   "outputs": [],
   "source": [
    "df.drop(columns='Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "AnO5jalf-rJx",
    "outputId": "bdf37be1-3c4f-494c-ec73-b0dc0455102a"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryXswm9c_h2f"
   },
   "source": [
    "### Replace missing values using [Imputer](https://sklearn.org/modules/generated/sklearn.preprocessing.Imputer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78dMygRdADPL"
   },
   "outputs": [],
   "source": [
    "df.replace(to_replace='?', value=np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "dvZ1pTDmYesV",
    "outputId": "12a32948-3456-4776-cb7d-04defa3c0071"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POfCU-16AT1H"
   },
   "source": [
    "We store the target to be predicted in Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_yUtLvaAfd6"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into feature(X) and target(Y)\n",
    "X = df.iloc[:,:-1].values\n",
    "Y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n0rdUX2BKNr"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "VCKpqCyCauvE",
    "outputId": "0dd840a5-e759-48c0-c159-181a7baa6ea5"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(imputer.fit_transform(df), dtype='float64')\n",
    "df.columns = ['WSR0','WSR1','WSR2','WSR3','WSR4','WSR5','WSR6','WSR7','WSR8','WSR9','WSR10','WSR11','WSR12','WSR13','WSR14','WSR15','WSR16','WSR17','WSR18','WSR19','WSR20','WSR21','WSR22','WSR23','WSR_PK','WSR_AV','T0','T1','T2','T3','T4','T5','T6','T7','T8','T9','T10','T11','T12','T13','T14','T15','T16','T17','T18','T19','T20','T21','T22','T23','T_PK','T_AV','T85','RH85','U85','V85','HT85','T70','RH70','U70','V70','HT70','T50','RH50','U50','V50','HT50','KI','TT','SLP','SLP_','Precp','Result']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "f8imU37wZn6e",
    "outputId": "271a40ab-579b-49df-b017-fa364463abfc"
   },
   "outputs": [],
   "source": [
    "# check if NaN values exist\n",
    "if np.nan in df['T_PK'].values.tolist():\n",
    "  print(\"NaN values found\")\n",
    "else:\n",
    "  print(\"No NaN values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2AhxKK5Bduy"
   },
   "outputs": [],
   "source": [
    "# Cleaning data using Imputer Class\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GwXCmuWB3-1"
   },
   "source": [
    "Now, we split the data into two parts â€” train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yv4cJhTB2JH"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPyPlFfPUq9F"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1IxEpT3UvWJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5WNxH_PfSht"
   },
   "source": [
    "### Peak Temperatures plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "Uerlx17pV_ZS",
    "outputId": "bfbec66a-3640-430e-da23-0aa5568cc7af"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (14,4))\n",
    "title = fig.suptitle(\"Peak Temperatures\", fontsize=14)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "# Histogram\n",
    "ax = fig.add_subplot(1,2, 1)\n",
    "ax.set_xlabel(\"Temperature\")\n",
    "ax.set_ylabel(\"Frequency\") \n",
    "ax.text(1.2, 800, r'$\\mu$='+str(round(df['T_PK'].mean(),2)), \n",
    "         fontsize=12)\n",
    "freq, bins, patches = ax.hist(df['T_PK'], color='steelblue', bins=15,\n",
    "                                    edgecolor='black', linewidth=1)\n",
    "\n",
    "\n",
    "# Density Plot\n",
    "ax1 = fig.add_subplot(1,2, 2)\n",
    "ax1.set_xlabel(\"Temperature\")\n",
    "ax1.set_ylabel(\"Frequency\") \n",
    "sns.kdeplot(df['T_PK'], ax=ax1, shade=True, color='steelblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chn9dKu_hZ7C"
   },
   "source": [
    "### Peak Wind Speed Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "id": "Fxm2ID8RhfUi",
    "outputId": "f8e776ad-4068-48df-8cd6-98943f1e37c9"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (14,4))\n",
    "title = fig.suptitle(\"Peak WSR\", fontsize=14)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "# Histogram\n",
    "ax = fig.add_subplot(1,2, 1)\n",
    "ax.set_xlabel(\"Wind Speed\")\n",
    "ax.set_ylabel(\"Frequency\") \n",
    "ax.text(1.2, 800, r'$\\mu$='+str(round(df['WSR_PK'].mean(),2)), \n",
    "         fontsize=12)\n",
    "freq, bins, patches = ax.hist(df['WSR_PK'], color='steelblue', bins=15,\n",
    "                                    edgecolor='black', linewidth=1)\n",
    "\n",
    "\n",
    "# Density Plot\n",
    "ax1 = fig.add_subplot(1,2, 2)\n",
    "ax1.set_xlabel(\"Wind Speed\")\n",
    "ax1.set_ylabel(\"Frequency\") \n",
    "sns.kdeplot(df['T_PK'], ax=ax1, shade=True, color='steelblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbra-zifiJig"
   },
   "source": [
    "### Correlation Matrix\n",
    "\n",
    "Here we depict the pair-wise correlation matrix as a heatmap, for a few attributes of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cormap = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(50,50))\n",
    "sns.heatmap(cormap,cmap=\"YlGnBu\", annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "6bHdlvD_iOke",
    "outputId": "eb16e5d4-f8fa-4a2a-bf35-eeb016190c4e"
   },
   "outputs": [],
   "source": [
    "# Correlation Matrix Heatmap\n",
    "f, ax = plt.subplots(figsize=(10, 6))\n",
    "subset_attributes = ['WSR_PK','T_PK','T85','T70','RH70','U70','V70','HT70','KI','TT','SLP','SLP_','Precp']\n",
    "corr = df[subset_attributes].corr()\n",
    "hm = sns.heatmap(round(corr,2), annot=True, ax=ax, cmap=\"coolwarm\",fmt='.2f',\n",
    "                 linewidths=.05)\n",
    "f.subplots_adjust(top=0.93)\n",
    "t= f.suptitle('Ozone Level Attributes Correlation Heatmap', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uydEO6sPkxI3"
   },
   "source": [
    "### Factorplot\n",
    "\n",
    "Here we try to establish a relation between peak temperature and the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slCBM2GLlaR2"
   },
   "outputs": [],
   "source": [
    "df['ozone_label'] = df['Result'].apply(lambda value: 'Non Ozone Day' if value == 0 else 'Ozone Day')\n",
    "df['ozone_label'] = pd.Categorical(df['ozone_label'], categories=['Non Ozone Day', 'Ozone Day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "1CGtat0EkyLm",
    "outputId": "33034a8c-44a5-4b46-afeb-870fe863e5d6"
   },
   "outputs": [],
   "source": [
    "# Visualizing 3-D categorical data using bar plots\n",
    "# leveraging the concepts of hue and facets\n",
    "fc = sns.factorplot(x=\"T_PK\", hue=\"Result\", col=\"ozone_label\", \n",
    "                    data=df, kind=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Qk4Se4OeeHY"
   },
   "source": [
    "## Using Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6RpL-y2e5mw"
   },
   "source": [
    "### Generating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EEVC1wWe3HA"
   },
   "outputs": [],
   "source": [
    "# Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "# Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "Y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjSRhkjffmt8"
   },
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "XKpuvsn9frd7",
    "outputId": "4bc4ef4f-556a-4c09-b444-13fe6cd7d048"
   },
   "outputs": [],
   "source": [
    "# Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\", metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(Y_test, Y_pred)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(mat, annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uCpLz0MgJiE"
   },
   "source": [
    "## Using K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rt4xhmRFszot"
   },
   "source": [
    "### Generating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2r6uBaJns53O"
   },
   "outputs": [],
   "source": [
    "# Import knearest neighbors Classifier model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model using the training sets\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "Y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aeSLqEQtC3V"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-6jx2qqptGys",
    "outputId": "d8903f85-b4a5-4f6e-f9a0-66b315b16275"
   },
   "outputs": [],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(Y_test, Y_pred)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "sns.heatmap(mat, annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2n2O0q30M5o"
   },
   "source": [
    "## Using Deep Neural Network\n",
    "\n",
    "We use [Keras](https://keras.io/) to implement the Neural Network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NcUTYuN54TiO",
    "outputId": "c05d91f6-1826-4a98-d893-a34e5bbb46fb"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYcgz9OsPOnu"
   },
   "source": [
    "Before feeding the data into the model, we standardise it. \n",
    "\n",
    "The StandardScaler assumes that the data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1.\n",
    "\n",
    "The mean and standard deviation are calculated for the feature and then the feature is scaled based on:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\left(\\frac{xiâ€“mean(x)}{stdev(x)}\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Q9VSBLuLgCt"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test  = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8TCSlBmCaNJ"
   },
   "source": [
    "The following code defines the model. There are 72 nodes in the input layer, followed by 100 and 50 nodes in the hidden layers, both followed by a ReLU activation unit, and then an output layer followed by Sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "5pb3XL350RNX",
    "outputId": "64741cbe-51d3-45aa-e1e6-fa84a652b335"
   },
   "outputs": [],
   "source": [
    "# Initializer that generates tensors with a normal distribution\n",
    "initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=42)\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, kernel_initializer=initializer, input_dim=72, activation= \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(50, kernel_initializer=initializer, activation= \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, kernel_initializer=initializer, activation= \"sigmoid\"))\n",
    "\n",
    "# Print model Summary\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzykC1MGDJMM"
   },
   "source": [
    "We calculate the Cross Entropy loss for this model, and print out the accuracy. The optimizer used is Adam, with a learning rate of 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ve8AOhV1355X"
   },
   "outputs": [],
   "source": [
    "# Set up the optimizer\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"binary_crossentropy\" , optimizer=adam, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "dLNYFo8k3-oB",
    "outputId": "22dbf727-5249-4608-e14f-2f63eb67157e"
   },
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "history = model.fit(X_train, Y_train, validation_split=0.25, epochs=10, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "n9ITIr0b4kXk",
    "outputId": "6fd8cd6c-0a21-4c3f-a8e9-16c9664fe1e8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "score = np.sqrt(mean_squared_error(Y_test, pred))\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZodcROxF5uN"
   },
   "source": [
    "### Keras Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "if 'accuracy' in history.history:\n",
    "    plt.plot(history.history['accuracy'])\n",
    "if 'val_accuracy' in history.history:\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "if 'loss' in history.history:\n",
    "    plt.plot(history.history['loss'])\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoongsnzpckC"
   },
   "source": [
    "## Accuracies\n",
    "\n",
    "SVM: 0.94%\n",
    "\n",
    "KNN: 93.68%\n",
    "\n",
    "DNN: 95.59%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DSDA Project â€” Ozone Level Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
